{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:43.545942100Z",
     "start_time": "2023-11-20T15:45:43.526485700Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/zulqarnainali/explained-singel-model-optiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# 📦 Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:43.768625400Z",
     "start_time": "2023-11-20T15:45:43.545942100Z"
    }
   },
   "id": "4b007a636cc52874"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 🤐 Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# 📊 Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = False  # Flag for inference mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:43.784304700Z",
     "start_time": "2023-11-20T15:45:43.768625400Z"
    }
   },
   "id": "ce46cc9508c97d21"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "far : 1.0017128691026516, near : 0.999660106068129\n"
     ]
    },
    {
     "data": {
      "text/plain": "(5237760, 17)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📂 Read the dataset from a CSV file using Pandas\n",
    "data_path = '../../data/'\n",
    "df = pd.read_csv(data_path + 'train.csv')\n",
    "\n",
    "# 🧹 Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "# null 값 처리\n",
    "far_price_mean = df['far_price'].mean()\n",
    "near_price_mean = df['near_price'].mean()\n",
    "print(f'far : {far_price_mean}, near : {near_price_mean}')\n",
    "# df['far_price'] = df['far_price'].fillna(far_price_mean)\n",
    "# df['near_price'] = df['near_price'].fillna(near_price_mean)\n",
    "df['far_price'] = df['far_price'].fillna(1)\n",
    "df['near_price'] = df['near_price'].fillna(0)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 📏 Get the shape of the DataFrame (number of rows and columns)\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:51.056413300Z",
     "start_time": "2023-11-20T15:45:43.784304700Z"
    }
   },
   "id": "20287b05cdfd3d11"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 🧹 Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df_, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 📏 Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df_.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # 🔄 Iterate through each column in the DataFrame\n",
    "    for col in df_.columns:\n",
    "        col_type = df_[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df_[col].min()\n",
    "            c_max = df_[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df_[col] = df_[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df_[col] = df_[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df_[col] = df_[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df_[col] = df_[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df_[col] = df_[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df_[col] = df_[col].astype(np.float32)\n",
    "                else:\n",
    "                    df_[col] = df_[col].astype(np.float32)\n",
    "\n",
    "    # ℹ️ Provide memory optimization information if 'verbose' is True\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df_.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # 🔄 Return the DataFrame with optimized memory usage\n",
    "    return df_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:51.103352900Z",
     "start_time": "2023-11-20T15:45:51.056413300Z"
    }
   },
   "id": "4bb82af2e0136e7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parallel Triplet Imbalance Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b184ea28544b1738"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # todo\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df_):\n",
    "    df_values = df_[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:51.330106400Z",
     "start_time": "2023-11-20T15:45:51.072029100Z"
    }
   },
   "id": "e0780db33a4d1df1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Generation Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf624355da2b062"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def imbalance_features(df_:pd.DataFrame):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df_[\"volume\"] = df_.eval(\"ask_size + bid_size\")\n",
    "    df_[\"mid_price\"] = df_.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df_[\"liquidity_imbalance\"] = df_.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df_[\"matched_imbalance\"] = df_.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df_[\"size_imbalance\"] = df_.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df_[f\"{c[0]}_{c[1]}_imb\"] = df_.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df_)\n",
    "        df_[triplet_feature.columns] = triplet_feature.values\n",
    "   \n",
    "    df_[\"imbalance_momentum\"] = df_.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df_['matched_size']\n",
    "    df_[\"price_spread\"] = df_[\"ask_price\"] - df_[\"bid_price\"]\n",
    "    df_[\"spread_intensity\"] = df_.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df_['price_pressure'] = df_['imbalance_size'] * (df_['ask_price'] - df_['bid_price'])\n",
    "    df_['market_urgency'] = df_['price_spread'] * df_['liquidity_imbalance']\n",
    "    df_['depth_pressure'] = (df_['ask_size'] - df_['bid_size']) * (df_['far_price'] - df_['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df_[f\"all_prices_{func}\"] = df_[prices].agg(func, axis=1)\n",
    "        df_[f\"all_sizes_{func}\"] = df_[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df_[f\"{col}_shift_{window}\"] = df_.groupby('stock_id')[col].shift(window)\n",
    "            df_[f\"{col}_ret_{window}\"] = df_.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df_[f\"{col}_diff_{window}\"] = df_.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df_.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def other_features(df_):\n",
    "    df_[\"dow\"] = df_[\"date_id\"] % 5  # Day of the week\n",
    "    df_[\"seconds\"] = df_[\"seconds_in_bucket\"] % 60  \n",
    "    df_[\"minute\"] = df_[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df_[f\"global_{key}\"] = df_[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df_\n",
    "\n",
    "def generate_all_features(df_):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df_.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df_ = df_[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df_ = imbalance_features(df_)\n",
    "    df_ = other_features(df_)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df_.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    return df_[feature_name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:45:51.346291100Z",
     "start_time": "2023-11-20T15:45:51.330106400Z"
    }
   },
   "id": "b666bd8a027fc8b7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "global_stock_id_feats = {\n",
    "    \"median_size\": df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "    \"std_size\": df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "    # \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "    \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"ask_size\"].min(),\n",
    "    \"median_price\": df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "    \"std_price\": df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "    \"ptp_price\": df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "}\n",
    "\n",
    "y_all = df[['target']].values\n",
    "df_train_feats = generate_all_features(df)\n",
    "df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "cols_group_by = ['stock_id', 'imbalance_buy_sell_flag']\n",
    "train_grouped_median = df_train_feats.groupby(cols_group_by).transform('median')\n",
    "df_train_feats = df_train_feats.fillna(train_grouped_median)\n",
    "df_train_feats = reduce_mem_usage(df_train_feats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:02.691999800Z",
     "start_time": "2023-11-20T15:45:51.346291100Z"
    }
   },
   "id": "5a6ea818d706560a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(5237760, 139)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:02.751892600Z",
     "start_time": "2023-11-20T15:48:02.657743300Z"
    }
   },
   "id": "f8d468022a8c0935"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "stock_id                                     0\nask_price_diff_1                             0\nimbalance_buy_sell_flag_shift_3              0\nimbalance_buy_sell_flag_ret_3                0\nimbalance_buy_sell_flag_shift_5              0\n                                            ..\nbid_size_ask_size_imbalance_size_imb2        0\nmatched_size_ask_size_imbalance_size_imb2    0\nmatched_size_bid_size_imbalance_size_imb2    0\nmatched_size_bid_size_ask_size_imb2          0\nglobal_ptp_price                             0\nLength: 139, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.isnull().sum(axis=0).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:03.494582600Z",
     "start_time": "2023-11-20T15:48:02.691999800Z"
    }
   },
   "id": "d254d90ef9e7f532"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(5237760, 139)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:03.494582600Z",
     "start_time": "2023-11-20T15:48:03.253889100Z"
    }
   },
   "id": "64c3c92f42ef91ac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['stock_id', 'seconds_in_bucket', 'imbalance_size',\n       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n       'far_price', 'near_price', 'bid_price', 'bid_size',\n       ...\n       'size_imbalance_diff_10', 'dow', 'seconds', 'minute',\n       'global_median_size', 'global_std_size', 'global_ptp_size',\n       'global_median_price', 'global_std_price', 'global_ptp_price'],\n      dtype='object', length=139)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:03.494582600Z",
     "start_time": "2023-11-20T15:48:03.269513900Z"
    }
   },
   "id": "7bc0b2e82ba16847"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(df_train_feats.values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:14.164051200Z",
     "start_time": "2023-11-20T15:48:03.285147500Z"
    }
   },
   "id": "486dbc5d7d823f02"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:26.060307Z",
     "start_time": "2023-11-20T15:48:14.158158Z"
    }
   },
   "id": "1a5eedad04b69daf"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision.transforms import transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.278549100Z",
     "start_time": "2023-11-20T15:48:26.060307Z"
    }
   },
   "id": "264ea7828933dfa7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 장비 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.389749700Z",
     "start_time": "2023-11-20T15:48:30.278549100Z"
    }
   },
   "id": "58a9f18e76fa1786"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "valid_dataset = TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.657338600Z",
     "start_time": "2023-11-20T15:48:30.389749700Z"
    }
   },
   "id": "b5ea02e03ef08775"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "batch_size=2048\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.657338600Z",
     "start_time": "2023-11-20T15:48:30.421188Z"
    }
   },
   "id": "761956a8c31f34b3"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.672949900Z",
     "start_time": "2023-11-20T15:48:30.437681Z"
    }
   },
   "id": "fa427cc8e6b8fe1a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# early stopper\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.00001):\n",
    "        self.best_model = None\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        \n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def early_stop(self, validation_loss, model):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            print(f\"New best loss: {validation_loss:>4f}\")\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.672949900Z",
     "start_time": "2023-11-20T15:48:30.453045900Z"
    }
   },
   "id": "3481059198b0a5e2"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "input_size = df_train_feats.shape[1]\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn0 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.fc1 = nn.Linear(in_features=1024, out_features=512) \n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=64)\n",
    "        self.fc5 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=32)\n",
    "        self.fc6 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.fc7 = nn.Linear(in_features=16, out_features=1) \n",
    "        self.relu = nn.LeakyReLU() # activation layer\n",
    "        self.dropout40 = nn.Dropout(p=0.4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn0(self.fc0(x)))\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout40(self.relu(self.bn5(self.fc5(x))))\n",
    "        x = self.dropout40(self.relu(self.fc6(x)))\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.672949900Z",
     "start_time": "2023-11-20T15:48:30.468676Z"
    }
   },
   "id": "66884662e6f4483e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "DNNModel(\n  (fc0): Linear(in_features=139, out_features=1024, bias=True)\n  (bn0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=256, out_features=128, bias=True)\n  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc4): Linear(in_features=128, out_features=64, bias=True)\n  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc5): Linear(in_features=64, out_features=32, bias=True)\n  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc6): Linear(in_features=32, out_features=16, bias=True)\n  (fc7): Linear(in_features=16, out_features=1, bias=True)\n  (relu): LeakyReLU(negative_slope=0.01)\n  (dropout40): Dropout(p=0.4, inplace=False)\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model = DNNModel().to(device)\n",
    "dnn_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.677250700Z",
     "start_time": "2023-11-20T15:48:30.484301Z"
    }
   },
   "id": "682b486dbf2ddbf1"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 1000\n",
    "lr_list = []\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, early_stopper, lr_scheduler=None, epochs=100, save_file=data_path+'model_state_dict.pth'):\n",
    "    train_epoch_loss = []\n",
    "    valid_epoch_loss = []\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        train_iter_loss = []\n",
    "        model.train()\n",
    "        for idx, (stocks, movements) in enumerate(train_loader):\n",
    "            stocks = stocks.to(device)\n",
    "            movements = movements.to(device)\n",
    "            outputs = model(stocks)\n",
    "            loss = criterion(outputs, movements)\n",
    "            train_iter_loss.append(loss.item())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()\n",
    "            if ((epoch+1)*idx)%10 == 0:\n",
    "                lr_list.append(optimizer.param_groups[0]['lr'])\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - Train loss : {sum(train_iter_loss)/len(train_loader):.4f}')\n",
    "        train_epoch_loss.append(sum(train_iter_loss)/len(train_loader))\n",
    "    \n",
    "        model.eval()\n",
    "        valid_iter_loss = []\n",
    "        for idx, (stocks, movements) in enumerate(valid_loader):\n",
    "            with torch.no_grad():\n",
    "                stocks = stocks.to(device)\n",
    "                movements = movements.to(device)\n",
    "                outputs = model(stocks)\n",
    "                loss = criterion(outputs, movements)\n",
    "                valid_iter_loss.append(loss.item())\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - Valid loss : {sum(valid_iter_loss)/len(valid_loader):.4f}')\n",
    "        valid_epoch_loss.append(sum(valid_iter_loss)/len(valid_loader))\n",
    "        if early_stopper.early_stop(sum(valid_iter_loss)/len(valid_loader), model):\n",
    "            torch.save(early_stopper.get_best_model().state_dict(), save_file)\n",
    "            break\n",
    "        gc.collect()\n",
    "    return torch.load(save_file), train_epoch_loss, valid_epoch_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.677250700Z",
     "start_time": "2023-11-20T15:48:30.657338600Z"
    }
   },
   "id": "36ec4e3f57d2817c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "criterion = nn.L1Loss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.677250700Z",
     "start_time": "2023-11-20T15:48:30.677250700Z"
    }
   },
   "id": "9b37fe60382ec9a9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 옵티마이저\n",
    "# optimizer = torch.optim.SGD(dnn_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.771678900Z",
     "start_time": "2023-11-20T15:48:30.677250700Z"
    }
   },
   "id": "a77321d3ac8d5d26"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# lr_scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps=100, cycle_mult=1.0, max_lr=0.1, min_lr=0.001, warmup_steps=20, gamma=0.5)\n",
    "# lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=100, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=len(train_loader)*10, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "# lr_scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=150,T_mult=1,eta_min=0.001)\n",
    "# lr_scheduler = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.787291500Z",
     "start_time": "2023-11-20T15:48:30.692794600Z"
    }
   },
   "id": "33361b5ca9a2728b"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=15, min_delta=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T15:48:30.787291500Z",
     "start_time": "2023-11-20T15:48:30.708430800Z"
    }
   },
   "id": "4f7139ac0cc15f04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Train loss : 6.3142\n",
      "Epoch [1/1000] - Valid loss : 6.2738\n",
      "New best loss: 6.273792\n",
      "Epoch [2/1000] - Train loss : 6.2942\n",
      "Epoch [2/1000] - Valid loss : 6.2575\n",
      "New best loss: 6.257494\n",
      "Epoch [3/1000] - Train loss : 6.2827\n",
      "Epoch [3/1000] - Valid loss : 6.2586\n",
      "Epoch [4/1000] - Train loss : 6.2759\n",
      "Epoch [4/1000] - Valid loss : 6.2415\n",
      "New best loss: 6.241505\n",
      "Epoch [5/1000] - Train loss : 6.2660\n",
      "Epoch [5/1000] - Valid loss : 6.2370\n",
      "New best loss: 6.236970\n",
      "Epoch [6/1000] - Train loss : 6.2576\n",
      "Epoch [6/1000] - Valid loss : 6.2360\n",
      "New best loss: 6.235962\n",
      "Epoch [7/1000] - Train loss : 6.2480\n",
      "Epoch [7/1000] - Valid loss : 6.2434\n",
      "Epoch [8/1000] - Train loss : 6.2389\n",
      "Epoch [8/1000] - Valid loss : 6.2223\n",
      "New best loss: 6.222326\n",
      "Epoch [9/1000] - Train loss : 6.2320\n",
      "Epoch [9/1000] - Valid loss : 6.2205\n",
      "New best loss: 6.220465\n",
      "Epoch [10/1000] - Train loss : 6.2288\n",
      "Epoch [10/1000] - Valid loss : 6.2282\n",
      "Epoch [11/1000] - Train loss : 6.2483\n",
      "Epoch [11/1000] - Valid loss : 6.2250\n",
      "Epoch [12/1000] - Train loss : 6.2452\n",
      "Epoch [12/1000] - Valid loss : 6.2252\n",
      "Epoch [13/1000] - Train loss : 6.2412\n",
      "Epoch [13/1000] - Valid loss : 6.2247\n",
      "Epoch [14/1000] - Train loss : 6.2355\n",
      "Epoch [14/1000] - Valid loss : 6.2339\n",
      "Epoch [15/1000] - Train loss : 6.2288\n",
      "Epoch [15/1000] - Valid loss : 6.2303\n",
      "Epoch [16/1000] - Train loss : 6.2211\n",
      "Epoch [16/1000] - Valid loss : 6.2136\n",
      "New best loss: 6.213595\n",
      "Epoch [17/1000] - Train loss : 6.2125\n",
      "Epoch [17/1000] - Valid loss : 6.2106\n",
      "New best loss: 6.210552\n",
      "Epoch [18/1000] - Train loss : 6.2062\n",
      "Epoch [18/1000] - Valid loss : 6.2116\n",
      "Epoch [19/1000] - Train loss : 6.2004\n",
      "Epoch [19/1000] - Valid loss : 6.2086\n",
      "New best loss: 6.208572\n",
      "Epoch [20/1000] - Train loss : 6.1969\n",
      "Epoch [20/1000] - Valid loss : 6.2080\n",
      "New best loss: 6.208039\n",
      "Epoch [21/1000] - Train loss : 6.2138\n"
     ]
    }
   ],
   "source": [
    "model_state_dict, train_losses, valid_losses = train(\n",
    "    model=dnn_model, train_loader=train_loader, valid_loader=valid_loader, criterion=criterion, \n",
    "    optimizer=optimizer, early_stopper=early_stopper, lr_scheduler=lr_scheduler, epochs=epochs\n",
    ")\n",
    "dnn_model.load_state_dict(model_state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-20T15:48:30.724398200Z"
    }
   },
   "id": "cb700a42baacce3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(lr_list)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "463766d57890b029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(train_losses)\n",
    "sns.lineplot(valid_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c4f3d734a20646c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
