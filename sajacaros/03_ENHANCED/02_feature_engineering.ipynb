{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:20.777798900Z",
     "start_time": "2023-11-15T07:49:20.773588100Z"
    }
   },
   "id": "2b5ed6c9420a3f31"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:20.792452800Z",
     "start_time": "2023-11-15T07:49:20.777798900Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/zulqarnainali/explained-singel-model-optiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# üì¶ Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import lightgbm as lgb  # LightGBM gradient boosting framework\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:21.074880100Z",
     "start_time": "2023-11-15T07:49:20.785752700Z"
    }
   },
   "id": "4b007a636cc52874"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# ü§ê Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# üìä Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = True  # Flag for inference mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:21.087814400Z",
     "start_time": "2023-11-15T07:49:21.074880100Z"
    }
   },
   "id": "ce46cc9508c97d21"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# üìÇ Read the dataset from a CSV file using Pandas\n",
    "data_path = '../../data/'\n",
    "df = pd.read_csv(data_path + 'train.csv')\n",
    "\n",
    "# üßπ Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# null Í∞í Ï≤òÎ¶¨\n",
    "df['far_price'] = df['far_price'].fillna(0)\n",
    "df['near_price'] = df['near_price'].fillna(1)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# cols_group_by = ['date_id', 'seconds_in_bucket']\n",
    "# cols_fill_nan = [\n",
    "#     'imbalance_size', 'reference_price', 'matched_size', 'wap',\n",
    "#     'bid_price', 'bid_size', 'ask_price', 'ask_size',\n",
    "#     'stock_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag']\n",
    "# train_grouped_median = df.groupby(cols_group_by)[cols_fill_nan].transform('median')\n",
    "# df[cols_fill_nan] = df[cols_fill_nan].fillna(train_grouped_median)\n",
    "\n",
    "# üìè Get the shape of the DataFrame (number of rows and columns)\n",
    "df_shape = df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:28.690060700Z",
     "start_time": "2023-11-15T07:49:21.087814400Z"
    }
   },
   "id": "20287b05cdfd3d11"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "stock_id                   0\ndate_id                    0\nseconds_in_bucket          0\nimbalance_size             0\nimbalance_buy_sell_flag    0\nreference_price            0\nmatched_size               0\nfar_price                  0\nnear_price                 0\nbid_price                  0\nbid_size                   0\nask_price                  0\nask_size                   0\nwap                        0\ntarget                     0\ntime_id                    0\nrow_id                     0\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:28.924239900Z",
     "start_time": "2023-11-15T07:49:28.691061300Z"
    }
   },
   "id": "6562d1754af2f393"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# üßπ Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # üìè Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # üîÑ Iterate through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # ‚ÑπÔ∏è Provide memory optimization information if 'verbose' is True\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # üîÑ Return the DataFrame with optimized memory usage\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:28.946390900Z",
     "start_time": "2023-11-15T07:49:28.933080200Z"
    }
   },
   "id": "4bb82af2e0136e7e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# üèéÔ∏è Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# üìä Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # üîÅ Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # üîÅ Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # üö´ Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / 1e-8\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# üìà Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:29.308727600Z",
     "start_time": "2023-11-15T07:49:28.946390900Z"
    }
   },
   "id": "e0780db33a4d1df1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# üìä Function to generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    # import cudf\n",
    "    # df = cudf.from_pandas(df)\n",
    "    \n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1 features\n",
    "    # Calculate various features using Pandas eval function\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    # V2 features\n",
    "    # Calculate additional features\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1).fillna(0) / (df['matched_size']+1e-8)\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff().fillna(0)\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    \n",
    "        \n",
    "    # V3 features\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        fill_value = df[col].mean()\n",
    "        for window in [1, 2, 3]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window).fillna(fill_value)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n",
    "        for window in [1, 2, 3]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window).fillna(0)\n",
    "    # df = df.to_pandas()\n",
    "    # Replace infinite values with 0\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def numba_imb_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    return df\n",
    "\n",
    "# üìÖ Function to generate time and stock-related features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n",
    "\n",
    "    # Map global features to the DataFrame\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# üöÄ Function to generate all features by combining imbalance and other features\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = numba_imb_features(df)\n",
    "    # Generate time and stock-related features\n",
    "    df = other_features(df)\n",
    "    gc.collect()  # Perform garbage collection to free up memory\n",
    "    \n",
    "    # Select and return the generated features\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:29.329900700Z",
     "start_time": "2023-11-15T07:49:29.309728200Z"
    }
   },
   "id": "b666bd8a027fc8b7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "# Check if the code is running in offline or online mode\n",
    "if is_offline:\n",
    "    # In offline mode, split the data into training and validation sets based on the split_day\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    \n",
    "    # Display a message indicating offline mode and the shapes of the training and validation sets\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    # In online mode, use the entire dataset for training\n",
    "    df_train = df\n",
    "    \n",
    "    # Display a message indicating online mode\n",
    "    print(\"Online mode\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:49:29.344094900Z",
     "start_time": "2023-11-15T07:49:29.333901200Z"
    }
   },
   "id": "57c26e16070edd39"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:00.520508600Z",
     "start_time": "2023-11-15T07:49:29.347518800Z"
    }
   },
   "id": "5a6ea818d706560a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['stock_id', 'seconds_in_bucket', 'imbalance_size',\n       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n       'ask_size', 'wap', 'volume', 'mid_price', 'liquidity_imbalance',\n       'matched_imbalance', 'size_imbalance', 'reference_price_far_price_imb',\n       'reference_price_near_price_imb', 'reference_price_ask_price_imb',\n       'reference_price_bid_price_imb', 'reference_price_wap_imb',\n       'far_price_near_price_imb', 'far_price_ask_price_imb',\n       'far_price_bid_price_imb', 'far_price_wap_imb',\n       'near_price_ask_price_imb', 'near_price_bid_price_imb',\n       'near_price_wap_imb', 'ask_price_bid_price_imb', 'ask_price_wap_imb',\n       'bid_price_wap_imb', 'imbalance_momentum', 'price_spread',\n       'spread_intensity', 'price_pressure', 'market_urgency',\n       'depth_pressure', 'matched_size_shift_1', 'matched_size_shift_2',\n       'matched_size_shift_3', 'imbalance_size_shift_1',\n       'imbalance_size_shift_2', 'imbalance_size_shift_3',\n       'reference_price_shift_1', 'reference_price_shift_2',\n       'reference_price_shift_3', 'imbalance_buy_sell_flag_shift_1',\n       'imbalance_buy_sell_flag_shift_2', 'imbalance_buy_sell_flag_shift_3',\n       'ask_price_diff_1', 'ask_price_diff_2', 'ask_price_diff_3',\n       'bid_price_diff_1', 'bid_price_diff_2', 'bid_price_diff_3',\n       'ask_size_diff_1', 'ask_size_diff_2', 'ask_size_diff_3',\n       'bid_size_diff_1', 'bid_size_diff_2', 'bid_size_diff_3',\n       'all_prices_mean', 'all_sizes_mean', 'all_prices_std', 'all_sizes_std',\n       'all_prices_skew', 'all_sizes_skew', 'all_prices_kurt',\n       'all_sizes_kurt', 'ask_price_bid_price_wap_imb2',\n       'ask_price_bid_price_reference_price_imb2',\n       'ask_price_wap_reference_price_imb2',\n       'bid_price_wap_reference_price_imb2',\n       'matched_size_bid_size_ask_size_imb2',\n       'matched_size_bid_size_imbalance_size_imb2',\n       'matched_size_ask_size_imbalance_size_imb2',\n       'bid_size_ask_size_imbalance_size_imb2', 'dow', 'seconds', 'minute',\n       'global_median_size', 'global_std_size', 'global_ptp_size',\n       'global_median_price', 'global_std_price', 'global_ptp_price'],\n      dtype='object')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:00.530137700Z",
     "start_time": "2023-11-15T07:50:00.530137700Z"
    }
   },
   "id": "7bc0b2e82ba16847"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(5237760, 88)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:00.545864500Z",
     "start_time": "2023-11-15T07:50:00.530137700Z"
    }
   },
   "id": "64c3c92f42ef91ac"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "stock_id             0\nseconds_in_bucket    0\nall_sizes_mean       0\nall_prices_mean      0\nbid_size_diff_3      0\nbid_size_diff_2      0\nbid_size_diff_1      0\nask_size_diff_3      0\ndtype: int64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.isnull().sum(axis=0).sort_values(ascending=False)[:8]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:01.020871400Z",
     "start_time": "2023-11-15T07:50:00.545864500Z"
    }
   },
   "id": "d254d90ef9e7f532"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_all = scaler.fit_transform(df_train_feats.values)\n",
    "# X_all = train_df[selected_features].values\n",
    "y_all = df_train[['target']].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:02.807079700Z",
     "start_time": "2023-11-15T07:50:01.020871400Z"
    }
   },
   "id": "486dbc5d7d823f02"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:10.068061400Z",
     "start_time": "2023-11-15T07:50:02.807079700Z"
    }
   },
   "id": "1a5eedad04b69daf"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision.transforms import transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:13.677094100Z",
     "start_time": "2023-11-15T07:50:10.068061400Z"
    }
   },
   "id": "264ea7828933dfa7"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ïû•ÎπÑ ÏÑ§Ï†ï\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:13.759767400Z",
     "start_time": "2023-11-15T07:50:13.677094100Z"
    }
   },
   "id": "58a9f18e76fa1786"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:13.789978700Z",
     "start_time": "2023-11-15T07:50:13.759767400Z"
    }
   },
   "id": "b5ea02e03ef08775"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:13.805607Z",
     "start_time": "2023-11-15T07:50:13.789978700Z"
    }
   },
   "id": "761956a8c31f34b3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (fc1): Linear(in_features=88, out_features=512, bias=True)\n  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=256, out_features=128, bias=True)\n  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc4): Linear(in_features=128, out_features=64, bias=True)\n  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc5): Linear(in_features=64, out_features=32, bias=True)\n  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc6): Linear(in_features=32, out_features=1, bias=True)\n  (relu): LeakyReLU(negative_slope=0.01)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = df_train_feats.shape[1]\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=512) \n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=64)\n",
    "        self.fc5 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=32)\n",
    "        self.fc6 = nn.Linear(in_features=32, out_features=1) \n",
    "        self.relu = nn.LeakyReLU() # activation layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "model = Model().to(device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:14.009567900Z",
     "start_time": "2023-11-15T07:50:13.805607Z"
    }
   },
   "id": "66884662e6f4483e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# ÏÜêÏã§ Ìï®Ïàò\n",
    "criterion = nn.L1Loss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:14.055859900Z",
     "start_time": "2023-11-15T07:50:14.009567900Z"
    }
   },
   "id": "9b37fe60382ec9a9"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:14.058235Z",
     "start_time": "2023-11-15T07:50:14.024573300Z"
    }
   },
   "id": "a77321d3ac8d5d26"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.optim.lr_scheduler.CosineAnnealingWarmRestarts at 0x1f3b254a220>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer=optimizer,\n",
    "    T_0=20,\n",
    "    T_mult=1,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "lr_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T07:50:14.058235Z",
     "start_time": "2023-11-15T07:50:14.040218Z"
    }
   },
   "id": "935482509106b78a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9dce314824b4328acfa619f6d3cd0b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train loss : 6.2932\n",
      "Epoch [1/100] - Valid loss : 6.2782\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca86fa149e14b50889db6ece90a7939"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] - Train loss : 6.2748\n",
      "Epoch [2/100] - Valid loss : 6.2615\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3528c6ab2b9147a78b41dae4bdbb6f2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] - Train loss : 6.2660\n",
      "Epoch [3/100] - Valid loss : 6.2509\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d66a3e2bce904fd9b74df102be7eb3d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] - Train loss : 6.2598\n",
      "Epoch [4/100] - Valid loss : 6.2454\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3aaebc2be90e48f6a67cbeafcc3a0e9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] - Train loss : 6.2543\n",
      "Epoch [5/100] - Valid loss : 6.2426\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9995dc701bcf47e591a34e3cdd4154fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] - Train loss : 6.2501\n",
      "Epoch [6/100] - Valid loss : 6.2399\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2349280c28534d90a8ae8fdaf8c7c410"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] - Train loss : 6.2465\n",
      "Epoch [7/100] - Valid loss : 6.2366\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7773f04363f4c9cb6a5df46f950383c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] - Train loss : 6.2426\n",
      "Epoch [8/100] - Valid loss : 6.2353\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab95a6b9a4f54ece84174e464211879f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] - Train loss : 6.2397\n",
      "Epoch [9/100] - Valid loss : 6.2324\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d377a5b81c94a34aaf1a6eb61307694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] - Train loss : 6.2368\n",
      "Epoch [10/100] - Valid loss : 6.2346\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eac7bd6105be41d0b6783a04c57f5f75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] - Train loss : 6.2343\n",
      "Epoch [11/100] - Valid loss : 6.2316\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea847fd607fb4e818b6a2ee58f88517a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] - Train loss : 6.2318\n",
      "Epoch [12/100] - Valid loss : 6.2303\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2098a59acdc440748c8629b35c93ab23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] - Train loss : 6.2299\n",
      "Epoch [13/100] - Valid loss : 6.2347\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5989fe49d16e4e6c97674d6954259f7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] - Train loss : 6.2279\n",
      "Epoch [14/100] - Valid loss : 6.2289\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "757797061e1f4407b1c1e24de887e9af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] - Train loss : 6.2260\n",
      "Epoch [15/100] - Valid loss : 6.2337\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3199c971a1b140c2900722d460811308"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] - Train loss : 6.2240\n",
      "Epoch [16/100] - Valid loss : 6.2316\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7dc4de5f8c945cf9109a24fefc5adca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] - Train loss : 6.2228\n",
      "Epoch [17/100] - Valid loss : 6.2269\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22387b808fd740d4ae40fd9f3ae29f9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] - Train loss : 6.2217\n",
      "Epoch [18/100] - Valid loss : 6.2302\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc00e47430a74e0cb903e7aa8e991f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] - Train loss : 6.2204\n",
      "Epoch [19/100] - Valid loss : 6.2314\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47b038c5914e48629c05c4b9ce64817d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] - Train loss : 6.2195\n",
      "Epoch [20/100] - Valid loss : 6.2334\n"
     ]
    },
    {
     "data": {
      "text/plain": "Train Loop:   0%|          | 0/4092 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf66d840d5be46d5b309ba3d0e6d6dd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] - Train loss : 6.2185\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 100\n",
    "\n",
    "train_epoch_loss = []\n",
    "valid_epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_iter_loss = []\n",
    "    bar = tqdm(enumerate(train_loader), total = len(train_loader), desc='Train Loop')\n",
    "    model.train()\n",
    "    for idx, (stocks, movements) in bar:\n",
    "        # with torch.autocast(device_type='cuda', dtype=torch.float64):\n",
    "        stocks = stocks.to(device)\n",
    "        movements = movements.to(device)\n",
    "        outputs = model(stocks)\n",
    "        loss = criterion(outputs, movements)\n",
    "        train_iter_loss.append(loss.item())\n",
    "        optimizer.zero_grad() # Í∏∞Ïö∏Í∏∞ Ï¥àÍ∏∞Ìôî\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        bar.set_postfix(\n",
    "            Epoch = epoch,\n",
    "            Current_loss = loss.item(),\n",
    "            Train_loss = sum(train_iter_loss)/(idx+1),\n",
    "            LR = optimizer.param_groups[0]['lr'],\n",
    "        )\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - Train loss : {sum(train_iter_loss)/len(train_loader):.4f}')\n",
    "    train_epoch_loss.append(sum(train_iter_loss)/len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    valid_iter_loss = []\n",
    "    for idx, (stocks, movements) in enumerate(valid_loader):\n",
    "        # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        with torch.no_grad():\n",
    "            stocks = stocks.to(device)\n",
    "            movements = movements.to(device)\n",
    "            outputs = model(stocks)\n",
    "            loss = criterion(outputs, movements)\n",
    "            valid_iter_loss.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - Valid loss : {sum(valid_iter_loss)/len(valid_loader):.4f}')\n",
    "    valid_epoch_loss.append(sum(valid_iter_loss)/len(valid_loader))\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-15T07:50:14.058235Z"
    }
   },
   "id": "36ec4e3f57d2817c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.argmin(train_epoch_loss), min(train_epoch_loss), np.argmin(valid_epoch_loss), min(valid_epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e821ce1e97f6b83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(train_epoch_loss, label='train')\n",
    "sns.lineplot(valid_epoch_loss, label='valid')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "230ea6baa9125ba4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6f302aeaec09f716"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "258230a4290582a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
